---
title: "Lista 1 - Inferencia"
author: "Boris Fazio"
date: "2 de noviembre de 2017"
output: html_document
---
  
## oli
  
1. Sea $X_1,...,X_n$ una muestra aleatoria tal que $X \sim W(2,\frac{1}{\theta})$. Considerar los estimadores

$$\begin{align}
\hat\theta_1 =\frac{1}{n}\sum_{i=1}^nX_i^2&&y&&\hat\theta_2 = a(\bar X)^2
\end{align}$$

a) Verificar si $\hat\theta_1$ es un estimador insesgado de $\theta$

---
  
$$\begin{align}
\text{E}[\hat\theta_1] &= \text{E}\left[\frac{1}{n}\sum_{i=1}^nX_i^2\right]\\
&= \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i^2\right]\\
&= \frac{1}{n}\sum_{i=1}^n\left(\text{V}\left[X_i\right]+\text{E}\left[X_i\right]^2\right)\\
&= \frac{1}{n}\sum_{i=1}^n\left[\theta\left(\Gamma(2)-\Gamma(1.5)^2\right)+\left(\sqrt{\theta}\Gamma(1.5)\right)^2\right]\\
&= \frac{1}{n}\sum_{i=1}^n\theta\\
&= \theta
\end{align}$$

Por lo tanto, $\hat\theta_1$ es insesgado.

---

b) Hallar $a$ tal que $\hat\theta_2$ sea insesgado
  
---
  
$$\begin{align}
\text{E}[\hat\theta_2] &= \text{E}\left[a\bar X^2\right]\\
&= \frac{a}{n^2}\text{E}\left[\left(\sum_{i=1}^nX_i\right)^2\right]\\
&= \frac{a}{n^2}\text{E}\left[2\sum_{\substack{i=1\\j>i}}^{n-1}X_iX_j + \sum_{i=1}^nX_i^2\right]\\
&= \frac{a}{n^2}\left(2\sum_{\substack{i=1\\j>i}}^{n-1}\text{E}\left[X_iX_j\right] + \sum_{i=1}^n\text{E}\left[X_i^2\right]\right)\\
&\text{Dado que la muestra es aleatoria, E}\left[X_iX_j\right] = \text{E}\left[X_i\right]\text{E}\left[X_j\right]\\
&= \frac{a}{n^2}\left(2\sum_{i=1}^{n(n-1)\over 2}\text{E}\left[X\right]^2 + \sum_{i=1}^n\left(\text{V}\left[X\right]+\text{E}\left[X\right]^2\right)\right)\\
&= \frac{a}{n}\left(\text{V}\left[X\right]+n\text{E}\left[X\right]^2\right)\\
&= \frac{a}{n}\left[\theta\left(\Gamma(2)-\Gamma(1.5)^2\right)+n\left(\sqrt{\theta}\Gamma(1.5)\right)^2\right]\\
&= a\theta\left(\frac{4+(n-1)\pi}{4n}\right)
\end{align}$$

Entonces $\hat\theta_2$ sera insesgado para $a = \frac{4n}{4+(n-1)\pi}$.

---

c) Si $\hat\theta_2$ es el estimador insesgado hallado en b), mostrar que que todo elemento de $\{\hat\theta_c = c\hat\theta_1 + (1-c)\hat\theta_2\}_{c\in [0,1]}$ es un estimador insesgado de $\theta$.

---

$$\begin{align}
\text{E}[\hat\theta_c] &= \text{E}\left[c\hat\theta_1 + (1-c)\hat\theta_2\right]\\
&= c\text{E}\left[\hat\theta_1\right] + (1-c)\text{E}\left[\hat\theta_2\right]\\
&= c\theta + (1-c)\theta\\
&= \theta
\end{align}$$

Entonces $\hat\theta_c$ es insesgado para cualquier valor de $c$.

---

d) Estudiar la consistencia fuerte de $\hat\theta_1$ y $\hat\theta_2$

---

https://math.stackexchange.com/questions/2166663/strong-consistency-of-estimator-2mean-on-uniform-distribution

$$\begin{align}
\text{P}(\lim_{n\rightarrow\infty}|\hat\theta_1-\theta| < \epsilon) = 
\end{align}$$

---

e) Hallar la varianza de $\hat\theta_1$

---

$$\begin{align}
\text{V}[\hat\theta_1] &= \text{E}\left[\left(\frac{1}{n}\sum_{i=1}^nX_i^2\right)^2\right]-\text{E}\left[\hat\theta_1\right]^2\\
&= \frac{1}{n^2}\left(\sum_{i=1}^n\text{E}\left[X_i^4\right] + 2\sum_{\substack{i=1\\j>i}}^{n-1}\text{E}\left[X_i^2X_j^2\right]\right)-\theta^2\\
&= \frac{1}{n}\left(\text{E}\left[X^4\right] + (n-1)\text{E}\left[X^2\right]^2\right)-\theta^2\\
&= \frac{1}{n}\left(\theta^2\Gamma\left(3\right) + (n-1)\theta^2\Gamma(2)^2\right)-\theta^2\\
&= \frac{\theta^2}{n}
\end{align}$$

---

f) Determinar si $\hat\theta_1$ es un estimador suficiente para $\theta$

---

Aplicamos la factorizacion de Fisher-Neyman sobre la verosimilitud:

$$\begin{align}
L(x_1,...,x_n\mid 2,\theta^{-1}) &= \prod_{i=1}^n2\theta^{-1} x_i\exp\{-\theta^{-1} x_i^2\}\\
&= \left(\prod_{i=1}^n2x_i\right)\left(\theta^{-n}\exp\{-\theta^{-1}\sum_{i=1}^nx_i^2\}\right)\\
&= h(x_{1...n})\left(\theta^{-n}\exp\{-n\theta^{-1}\hat\theta_1\}\right)
\end{align}$$

Entonces se verifica que $\hat\theta_1$ es un estimador suficiente de $\theta$.

---

g) Mostrar que $\frac{\sqrt{n}\left(\hat\theta_1-\theta\right)}{2\sqrt{\hat\theta_1}} \xrightarrow{\mathcal{D}} \text{N}(0,h)$ y determinar el valor de $h$

---

---

2. Considere una muestra aleatoria de $n$ observaciones de $X$ tal que

$$\begin{align}
f_X(x\mid\alpha)=
\begin{cases}
\alpha&,0<x\leq1\\
1-\alpha&,1<x\leq2\\
0&,\text{en otro caso}
\end{cases}
\end{align}$$

y un estadistico $\hat\alpha = a+b\sum_{i=1}^nX_i$

a) Determinar los valores de $a$ y $b$ para que $\hat\alpha$ sea un estimador insesgado de $\alpha$.

---

$$\begin{align}
\text{E}[\hat\alpha] &= \text{E}\left[a+b\sum_{i=1}^nX_i\right]\\
&= a+b\sum_{i=1}^n\text{E}\left[X_i\right]\\
&= a+bn(1.5-\alpha)
\end{align}$$

Con $a = 1.5$ y $b = -n^{-1}$, $\hat\alpha$ es insesgado.

---

b) cramer rao... chasawa???

---

3. Se tiene una muestra de $n$ observaciones de $X\sim\text{U}(0,2\theta)$

a) Halle el MELI de $\theta$

---

$$\begin{align}
\text{E}\left[\hat\theta_n\right] &= \text{E}\left[\sum_{i=1}^na_iX_i\right]\\
&= \sum_{i=1}^na_i\text{E}\left[X_i\right]\\
&= \theta\sum_{i=1}^na_i\\
\end{align}$$

Para obtener $\hat\theta_n$ insesgado, requerimos $\sum_{i=1}^na_i = 1$. Podemos expresar los $a_i$ en terminos de su valor medio y las desviaciones individuales, $\sum_{i=1}^n(\frac{1}{n}+d_i)$, lo que implica que $\sum_{i=1}^nd_i = 0$.

$$\begin{align}
\text{V}\left[\hat\theta_n\right] &= \text{V}\left[\sum_{i=1}^na_iX_i\right]\\
&= \sum_{i=1}^na_i^2\text{V}\left[X_i\right]\\
&= \frac{\theta^2}{3}\sum_{i=1}^n\left(\frac{1}{n} + d_i\right)^2\\
&= \frac{\theta^2}{3}\left(\sum_{i=1}^n\frac{1}{n^2} + \sum_{i=1}^n\frac{2d_i}{n} + \sum_{i=1}^nd_i^2\right)\\
&= \frac{\theta^2}{3}\left(\frac{1}{n} + \sum_{i=1}^nd_i^2\right)\\
\end{align}$$

Por lo tanto, la varianza es minima cuando todo $d_i = 0$, lo que implica $a_i = 1/n$. Entonces, el MELI viene dado por

$$\hat\theta_n^\star = \frac{1}{n}\sum_{i=1}^nX_i$$

---

b) Analice la consistencia de $\hat\theta_n^\star$

---

Dado que $\hat\theta_n^\star$ es insesgado y

$$\lim_{n \to \infty}V\left(\hat\theta_n^\star\right) = \lim_{n \to \infty}\theta^2/3n = 0$$

podemos aplicar Chebychev y verificamos que


$$\lim_{n\to\infty}\text{P}\left(|\hat\theta_n^\star - \theta| \geq \epsilon\right) = 0$$

---

c) Determine si el estimador insesgado $\tilde\theta = C\max \{X_1,...,X_n\}$ es mejor que el MELI

Hallamos $C$ para tener $\tilde \theta$ insesgado:

$$\begin{align}
\text{E}\left[\tilde\theta\right] &= C\text{E}\left[\max \{X_1,...,X_n\} \right]\\
&= C\int_{-\infty}^{\infty} x nf_X(x)F_X(x)^{(n-1)}dx\\
&= Cn\int_{0}^{2\theta} x \frac{1}{2\theta}\left(\frac{x}{2\theta}\right)^{(n-1)}dx\\
&= \frac{Cn}{\left(2\theta\right)^n}\int_{0}^{2\theta}x^ndx\\
&= \frac{Cn}{\left(2\theta\right)^n}\frac{\left(2\theta\right)^{n+1}}{n+1}\\
&= C\frac{2n}{n+1}\theta\\
\end{align}$$

Con $C = \frac{n+1}{2n}$ se tiene el estimador insesgado. Evaluamos su varianza:

$$\begin{align}
\text{V}\left[\tilde\theta\right] &= \text{E}\left[\tilde\theta^2 \right]-\text{E}\left[\tilde\theta \right]^2\\
&= C^2\int_{0}^{2\theta} x^2 n\frac{1}{2\theta}\left(\frac{x}{2\theta}\right)^{(n-1)}dx - \theta^2\\
&= C^2\frac{n(2\theta)^{n+2}}{\left(2\theta\right)^n(n+2)} - \theta^2\\
&= \theta^2\left(\frac{(n+1)^2}{4n^2}\frac{4n}{n+2} - 1\right)\\
&= \theta^2
\end{align}$$

Entonces este estimador no es mejor ya que su varianza es mas alta para cualquier $n$ que se tome.

---

4. Se toma una muestra de $n$ observaciones de $X \sim \text{B}(m,p)$ con $m$ conocido. Consideres los estimadores

$$\hat p_1 = \frac{\bar X}{m} \text{, } \hat p_2 = \bar X \text{, } \hat p_3 = \frac{X_1 + X_n}{2m} \text{ y } \hat p_4 = \frac{X_1}{m}$$

a) Determine cuales son insesgados y de ese grupo, el mas eficiente

---

$$\begin{align}
\text{E}[\hat p_1] &= \text{E}\left[\frac{\bar X}{m}\right]&\text{E}[\hat p_2] &= \text{E}\left[\bar X\right]&\text{E}[\hat p_3] &= \text{E}\left[\frac{X_1 + X_n}{2m}\right]&\text{E}[\hat p_4] &= \text{E}\left[\frac{X_1}{m}\right]\\
&= \frac{1}{mn}\sum_{i=1}^n\text{E}\left[X_i\right]&&= \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i\right]&&= \frac{1}{2m}\left(\text{E}\left[X_1\right]+\text{E}\left[X_n\right]\right)&&= \frac{1}{m}\text{E}\left[X_1\right]\\
&= \frac{1}{mn}\sum_{i=1}^nmp&&= \frac{1}{n}\sum_{i=1}^nmp&&= \frac{1}{2m}2mp&&= \frac{1}{m}mp\\
&= p&&= mp&&= p&&= p
\end{align}$$

Entonces $\hat p_2$ es el unico estimador bajo consideracion que no es insesgado.

$$\begin{align}
\text{V}[\hat p_1] &= \text{V}\left[\frac{\bar X}{m}\right]&\text{V}[\hat p_3] &= \text{V}\left[\frac{X_1 + X_n}{2m}\right]&\text{V}[\hat p_4] &= \text{V}\left[\frac{X_1}{m}\right]\\
&= \frac{1}{m^2n^2}\sum_{i=1}^n\text{V}\left[X_i\right]&&= \frac{1}{4m^2}\left(\text{V}\left[X_1\right]+\text{V}\left[X_n\right]\right)&&= \frac{1}{m^2}\text{V}\left[X_1\right]\\
&= \frac{1}{m^2n^2}\sum_{i=1}^nmp(1-p)&&= \frac{1}{4m^2}2mp(1-p)&&= \frac{1}{m^2}mp(1-p)\\
&= \frac{p(1-p)}{nm}&&= \frac{p(1-p)}{2m}&&= \frac{p(1-p)}{m}
\end{align}$$

La varianza de $\hat p_1$ es menor para $n > 2$, por lo que siempre es tan o mas eficiente que los otros estimadores bajo consideracion.

---

b) Dado $p = 0.8$, $n = 10$ y $m = 1$, calcule la probabilidad de que $\hat p_1$ produzca un error de estimacion no mayor a $0.1$.

---

$$\begin{align}
\text{P}\left(0.7 \leq \hat p_1 \leq 0.9\right)_{m = 1, p = 0.8; n = 10} &= \text{P}\left(0.7 \leq \frac{1}{10}\sum_{i=1}^{10}X_i \leq 0.9 \right)\\
&= \sum_{j=7}^9\text{P}\left(\sum_{i=1}^{10}X_i = j \right)\\
&= \sum_{j=7}^9{10 \choose j}0.8^j(1-0.8)^{10-j}\\
&\approx 0.77 
\end{align}$$

---

c) Si $n$ es suficientemente grande, determine la distribucion aproximada de $\hat p_1$



https://www.math.unl.edu/~sdunbar1/ProbabilityTheory/Lessons/NormalGaussians/SumofNormals/sumofnormals.pdf
