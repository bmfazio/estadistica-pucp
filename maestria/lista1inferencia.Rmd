---
title: "Lista 1 - Inferencia"
author: "Boris Fazio"
date: "2 de noviembre de 2017"
output: html_document
---
  
## oli
  
1. wapo

$$\begin{align}
\text{E}\left[X\right] &= \beta^{-1/\alpha}\Gamma\left(1+\frac{1}{\alpha}\right)\\
&= \theta^{1/2}\Gamma\left(1.5\right) = \frac{1}{2}\sqrt{\theta\pi}\\
\text{V}\left[X\right] &= \beta^{-2/\alpha}\left(\Gamma\left(1+\frac{2}{\alpha}\right)-\Gamma\left(1+\frac{1}{\alpha}\right)^2\right)\\
&= \theta\left(\Gamma\left(2\right)-\Gamma\left(1.5\right)^2\right) = \theta\left(1-\frac{\pi}{4}\right)
\end{align}$$

$$\begin{align}
\hat\theta_1 &= \frac{1}{n}\sum_{i=1}^nX_i^2\\
\hat\theta_2 &= a(\bar X)^2
\end{align}$$
  
a) sesgo de $theta_1$
  
$$\begin{align}
\text{E}[\hat\theta_1] &= \text{E}\left[\frac{1}{n}\sum_{i=1}^nX_i^2\right]\\
&= \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i^2\right]\\
&= \frac{1}{n}\sum_{i=1}^n\left(\text{V}\left[X_i\right]+\text{E}\left[X_i\right]^2\right)\\
&\text{Reemplazamos con los valores de los parametros}\\
&= \frac{1}{n}\sum_{i=1}^n\left[\theta\left(\Gamma(2)-\Gamma(1.5)^2\right)+\left(\sqrt{\theta}\Gamma(1.5)\right)^2\right]\\
&= \frac{1}{n}\sum_{i=1}^n\theta\\
&= \theta
\end{align}$$

Por lo tanto, $\hat\theta_1$ es insesgado.

b) sesgo de $theta_2$
  
$$\begin{align}
\text{E}[\hat\theta_2] &= \text{E}\left[a\bar X^2\right]\\
&= \frac{a}{n^2}\text{E}\left[\left(\sum_{i=1}^nX_i\right)^2\right]\\
&= \frac{a}{n^2}\text{E}\left[2\sum_{\substack{i=1\\j>i}}^{n-1}X_iX_j + \sum_{i=1}^nX_i^2\right]\\
&= \frac{a}{n^2}\left(2\sum_{\substack{i=1\\j>i}}^{n-1}\text{E}\left[X_iX_j\right] + \sum_{i=1}^n\text{E}\left[X_i^2\right]\right)\\
&\text{Dado que la muestra es aleatoria, E}\left[X_iX_j\right] = \text{E}\left[X_i\right]\text{E}\left[X_j\right]\\
&= \frac{a}{n^2}\left(2\sum_{i=1}^{n(n-1)\over 2}\text{E}\left[X\right]^2 + \sum_{i=1}^n\left(\text{V}\left[X\right]+\text{E}\left[X\right]^2\right)\right)\\
&= \frac{a}{n}\left(\text{V}\left[X\right]+n\text{E}\left[X\right]^2\right)\\
&= \frac{a}{n}\left[\theta\left(\Gamma(2)-\Gamma(1.5)^2\right)+n\left(\sqrt{\theta}\Gamma(1.5)\right)^2\right]\\
&= a\theta\left(\frac{4+(n-1)\pi}{4n}\right)
\end{align}$$

$\hat\theta_2$ sera insesgado para $a = \frac{4n}{4+(n-1)\pi}$.

c) sesgo de $\hat\theta_c$

$$\begin{align}
\text{E}[\hat\theta_c] &= \text{E}\left[c\hat\theta_1 + (1-c)\hat\theta_2\right]\\
&= c\text{E}\left[\hat\theta_1\right] + (1-c)\text{E}\left[\hat\theta_2\right]\\
&= c\theta + (1-c)\theta\\
&= \theta
\end{align}$$

d) consistencia fuerte

e) varianza de $\hat\theta_1$

$$\begin{align}
\text{V}[\hat\theta_1] &= \text{E}\left[\left(\frac{1}{n}\sum_{i=1}^nX_i^2\right)^2\right]-\text{E}\left[\hat\theta_1\right]^2\\
&= \frac{1}{n^2}\left(\sum_{i=1}^n\text{E}\left[X_i^4\right] + 2\sum_{\substack{i=1\\j>i}}^{n-1}\text{E}\left[X_i^2X_j^2\right]\right)-\theta^2\\
&= \frac{1}{n}\left(\text{E}\left[X^4\right] + (n-1)\text{E}\left[X^2\right]^2\right)-\theta^2\\
&= \frac{1}{n}\left(\theta^2\Gamma\left(3\right) + (n-1)\theta^2\Gamma(2)^2\right)-\theta^2\\
&= \frac{\theta^2}{n}
\end{align}$$

f) g) wtf is des chet

2.

$\hat\alpha = a+b\sum_{i=1}^nX_i$

a) $a$ $b$ para insesgao

$$\begin{align}
\text{E}[\hat\alpha] &= \text{E}\left[a+b\sum_{i=1}^nX_i\right]\\
&= a+b\sum_{i=1}^n\text{E}\left[X_i\right]\\
&= a+bn(1.5-\alpha)
\end{align}$$

Con $a = 1.5$ y $b = -n^{-1}$, $\hat\alpha$ es insesgado.

b) cramer rao... chasawa???

3.

$X \sim \text{U}(0,2\theta)$

a) hallar MELI

$$\begin{align}
\text{E}\left[\hat\theta_n\right] &= \text{E}\left[\sum_{i=1}^na_iX_i\right]\\
&= \sum_{i=1}^na_i\text{E}\left[X_i\right]\\
&= \theta\sum_{i=1}^na_i\\
\end{align}$$

Para obtener $\hat\theta_n$ insesgado, requerimos $\sum_{i=1}^na_i = 1$. Podemos expresar los $a_i$ en terminos de su valor medio y las desviaciones individuales, $\sum_{i=1}^n(\frac{1}{n}+d_i)$, lo que implica que $\sum_{i=1}^nd_i = 0$.

$$\begin{align}
\text{V}\left[\hat\theta_n\right] &= \text{V}\left[\sum_{i=1}^na_iX_i\right]\\
&= \sum_{i=1}^na_i^2\text{V}\left[X_i\right]\\
&= \frac{\theta^2}{3}\sum_{i=1}^n\left(\frac{1}{n} + d_i\right)^2\\
&= \frac{\theta^2}{3}\left(\sum_{i=1}^n\frac{1}{n^2} + \sum_{i=1}^n\frac{2d_i}{n} + \sum_{i=1}^nd_i^2\right)\\
&= \frac{\theta^2}{3}\left(\frac{1}{n} + \sum_{i=1}^nd_i^2\right)\\
\end{align}$$

Por lo tanto, la varianza es minima cuando $d_i = 0$, lo que implica $a_i = 1/n$. Entonces, el MELI viene dado por

$$\hat\theta_n^\star = \frac{1}{n}\sum_{i=1}^nX_i$$

b) consistencia

Dado que $\hat\theta_n^\star$ es insesgado y

$$\lim_{n \to \infty}V\left(\hat\theta_n^\star\right) = \lim_{n \to \infty}\theta^2/3n = 0$$

podemos aplicar Chebychev y verificamos que


$$\lim_{n\to\infty}\text{P}\left(|\hat\theta_n^\star - \theta| \geq \epsilon\right) = 0$$

c) distrib maximo

Hallamos $C$ para tener $\tilde \theta$ insesgado:

$$\begin{align}
\text{E}\left[\tilde\theta\right] &= C\text{E}\left[\max \{X_1,...,X_n\} \right]\\
&= C\int_{-\infty}^{\infty} x nf_X(x)F_X(x)^{(n-1)}dx\\
&= Cn\int_{0}^{2\theta} x \frac{1}{2\theta}\left(\frac{x}{2\theta}\right)^{(n-1)}dx\\
&= \frac{Cn}{\left(2\theta\right)^n}\int_{0}^{2\theta}x^ndx\\
&= \frac{Cn}{\left(2\theta\right)^n}\frac{\left(2\theta\right)^{n+1}}{n+1}\\
&= C\frac{2n}{n+1}\theta\\
\end{align}$$

Con $C = \frac{n+1}{2n}$ se tiene el estimador insesgado. Evaluamos varianza:

$$\begin{align}
\text{V}\left[\tilde\theta\right] &= \text{E}\left[\tilde\theta^2 \right]-\text{E}\left[\tilde\theta \right]^2\\
&= C^2\int_{0}^{2\theta} x^2 \frac{1}{2\theta}\left(\frac{x}{2\theta}\right)^{(n-1)}dx - \theta^2\\
&= \frac{C^2}{\left(2\theta\right)^n}\frac{(2\theta)^{n+2}}{n+2} - \theta^2\\
\end{align}$$
4.

$$\hat p_1 = \frac{\bar X}{m} \text{, } \hat p_2 = \bar X \text{, } \hat p_3 = \frac{X_1 + X_n}{2m} \text{ y } \hat p_4 = \frac{X_1}{m}$$

a) cual es insesgau

$$\begin{align}
\text{E}[\hat p_1] &= \text{E}\left[\frac{\bar X}{m}\right]&\text{E}[\hat p_2] &= \text{E}\left[\bar X\right]&\text{E}[\hat p_3] &= \text{E}\left[\frac{X_1 + X_n}{2m}\right]&\text{E}[\hat p_4] &= \text{E}\left[\frac{X_1}{m}\right]\\
&= \frac{1}{mn}\sum_{i=1}^n\text{E}\left[X_i\right]&&= \frac{1}{n}\sum_{i=1}^n\text{E}\left[X_i\right]&&= \frac{1}{2m}\left(\text{E}\left[X_1\right]+\text{E}\left[X_n\right]\right)&&= \frac{1}{m}\text{E}\left[X_1\right]\\
&= \frac{1}{mn}\sum_{i=1}^nmp&&= \frac{1}{n}\sum_{i=1}^nmp&&= \frac{1}{2m}2mp&&= \frac{1}{m}mp\\
&= p&&= mp&&= p&&= p
\end{align}$$

evaluar eficiencia

$$\begin{align}
\text{V}[\hat p_1] &= \text{V}\left[\frac{\bar X}{m}\right]&\text{V}[\hat p_3] &= \text{V}\left[\frac{X_1 + X_n}{2m}\right]&\text{V}[\hat p_4] &= \text{V}\left[\frac{X_1}{m}\right]\\
&= \frac{1}{m^2n^2}\sum_{i=1}^n\text{V}\left[X_i\right]&&= \frac{1}{4m^2}\left(\text{V}\left[X_1\right]+\text{V}\left[X_n\right]\right)&&= \frac{1}{m^2}\text{V}\left[X_1\right]\\
&= \frac{1}{m^2n^2}\sum_{i=1}^nmp(1-p)&&= \frac{1}{4m^2}2mp(1-p)&&= \frac{1}{m^2}mp(1-p)\\
&= \frac{p(1-p)}{nn}&&= \frac{p(1-p)}{2m}&&= \frac{p(1-p)}{m}
\end{align}$$

La varianza de $\hat p_1$ es menor para $n > 2$, por lo que siempre es tan o mas eficiente que los otros estimadores bajo consideracion.

b) probabilidad maxima de error (probabilidad de 7, 8, 9)

$$\begin{align}
\text{P}\left(0.7 \leq \hat p_1 \leq 0.9\right)_{m = 1, p = 0.8; n = 10} &= \text{P}\left(0.7 \leq \frac{1}{10}\sum_{i=1}^{10}X_i \leq 0.9 \right)\\
&= \sum_{j=7}^9\text{P}\left(\sum_{i=1}^{10}X_i = j \right)\\
&= \sum_{j=7}^9{10 \choose j}0.8^j(1-0.8)^{10-j}\\
&\approx 0.77 
\end{align}$$

c) https://www.math.unl.edu/~sdunbar1/ProbabilityTheory/Lessons/NormalGaussians/SumofNormals/sumofnormals.pdf
